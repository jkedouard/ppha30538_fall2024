---
<<<<<<< HEAD
title: "Problem Set 1"
author: "Jennifer Edouard"
date: "October 6, 2024"
date-format: long
format: 
  html:
    echo: true
    toc: true
---
<!--
    beamer:
        echo: true
        toc: true
        aspectratio: 169
        theme: default
        header-includes: \renewcommand{\tightlist}{\setlength{\itemsep}{5ex}\setlength{\parskip}{0pt}}
            \setbeamertemplate{footline}[frame number] 
            -->


```{python}
import pandas as pd
```

```{python}
import altair as alt
```

## 1. Read in one percent sample (15 points)

1.1 To help you get started, we pushed a file to the course repo called parking_tickets_one_percent.csv which gives you a one percent sample of tickets. We constructed the sample by selecting 1 ticket numbers that end in 01. How long does it take to read in this file? (Find a function to measure how long it takes the command to run. Note that everytime you run, there will be some difference in how long the code takes to run). Add an assert statement which verifies that there are 287458 rows.

Sources: https://www.geeksforgeeks.org/how-to-check-the-execution-time-of-python-script/

```{python}
import time
```

```{python}
start = time.time()

base_path = r"C:\Users\jenni\OneDrive - The University of Chicago\2-Python II\Github\ppha30538_fall2024\problem_sets\ps1\data\parking_tickets_one_percent.csv"

df_ps1 = pd.read_csv(base_path)

end = time.time()

print("It took", end - start, "seconds to read in the data file")
```

```{python}
assert len(df_ps1) == 287458
```

1.2 Using a function in the os library calculate how many megabytes is the CSV file? Using math, how large would you predict the full data set is?

Sources: https://www.geeksforgeeks.org/get-file-size-in-bytes-kb-mb-and-gb-using-python/

```{python}
import os
```

```{python}
file_info = os.stat(base_path)
file_size_bytes = file_info.st_size

file_size_mb = file_size_bytes / (1024 * 1024)

print(f"The data set is about", file_size_mb, "megabytes")
```

Knowing that the data is only from ticket numbers ending in 01 and that there are 100 possible combinations of two digits, we can estimate that this data is merely 1% of the entire dataset. I'll multiply the size of this file by 100 to get a better sense of the larger dataset

```{python}
print(f"The larger data set is about", file_size_mb * 100, "megabytes")
```

1.3 The rows on the dataset are ordered or sorted by a certain column by default. Which column? Then, subset the dataset to the first 500 rows and write a function that tests if the column is ordered.

Sources: https://www.datacamp.com/tutorial/functions-python-tutorial

The rows seem to be in order of the issue_date column, which is the column stating the date of the ticket issuance

```{python}
first_500_subset = df_ps1.head(500)
```

```{python}
def ordered(data):
    return data.is_monotonic_increasing

check_yes = ordered(first_500_subset["issue_date"])

if check_yes:
    print("The ticket issuance date column is ordered.")
else: 
    print("The ticket issuance date column is not ordered")
```

## 2. Cleaning the data and benchmarking (15 points)

2.1 How many tickets were issued in the data in 2017? How many tickets does that imply were issued in the full data in 2017? How many tickets are issued each year according to the ProPublica article? Do you think that there is a meaningful difference?

```{python}
only_2017_subset = df_ps1[df_ps1["issue_date"].str.startswith("2017")]

print(f"According to our data inclduing only tickets ending in '01', in the year 2017, {len(only_2017_subset)} tickets were issued. However, this means that the estimate for all the tickets issued in 2017 is {len(only_2017_subset) * 100}. According to the ProPulica article, 'EACH YEAR, the City of Chicago issues more than 3 million tickets.' Based on that estimate, I would say there is a meaningful difference in our estimate compared to the ProPublica information.")
```

2.2 Pooling the data across all years what are the top 20 most frequent violation types?Make a bar graph to show the frequency of these ticket types. Format the graph such that the violation descriptions are legible and no words are cut off.

Sources: https://stackoverflow.com/questions/53983072/arrange-bar-chart-in-ascending-descending-order & https://vega.github.io/vega/tutorials/bar-chart/ 

```{python}
#| echo: true
#| eval: false
pip install vega_datasets
```

```{python}
import vega_datasets
```

```{python}
group_violation = df_ps1.groupby("violation_description").size().reset_index(name = "count")
group_violation = group_violation.sort_values(by = "count", ascending = False)

top_20_violation = group_violation.head(20)
```

```{python}
chart_1 = alt.Chart(top_20_violation).mark_bar().encode(
    alt.X("violation_description", title = "Traffic Violation Description", sort = alt.EncodingSortField(field = "count", order = "descending"),
    axis = alt.Axis(labelAngle = -30)
    ),
    alt.Y("count", title = "Frequency")
).properties(
    title = "Top 20 Most Frequently Ticketed Traffic Violations in Chicago, IL",
    width = 800
)
chart_1
```

## 3. Visual Encoding (15 points)

3.1 In lecture 2, we discussed how Altair thinks about categorizing data series into four different types. Which data type or types would you associate with each column in the data frame? Your response should take the form of a markdown table where each row corresponds to one of the variables in the parking tickets dataset, the first column is the variable name and the second column is the variable type or types. If you argue that a column might be associated with than one type, explain why in writing below the table

| Variable Name         | Variable Type(s)      |
|-----------------------|-----------------------|
| ticket_number | Ordinal
| issue_date | Temporal
| violation_location | Nominal
| license_plate_number | Nominal
| license_plate_state | Nominal
| license_plate_type | Nominal
| zipcode | Nominal
| violation_code | Nominal
| violation_description | Nominal
| unit | Nominal
| unit_description | Nominal
| vehicle_make | Nominal
| fine_level1_amount | Quantitative
| fine_level2_amount | Quantitative
| current_amount_due | Quantitative
| total_payments | Quantitative
| ticket_queue | Ordinal
| ticket_queue_date | Temporal
| notice_level | Nominal
| hearing_disposition | Nominal
| notice_number | Ordinal
| officer | Nominal
| address | Nominal

3.2 Compute the fraction of time that tickets issued to each vehicle make are marked as paid. Show the results as a bar graph. Why do you think that some vehicle makes are more or less likely to have paid tickets?

```{python}
group_vehicle_make = df_ps1[df_ps1["current_amount_due"] == "0"]

group_vehicle_make = df_ps1.groupby("vehicle_make").size().reset_index(name = "count")

group_vehicle_make["fraction_of_total"] = group_vehicle_make["count"] / group_vehicle_make["count"].sum()
```

```{python}
chart_2 = alt.Chart(group_vehicle_make, width = 1200, height = 500).mark_bar().encode(
    alt.X("vehicle_make", title = "Vehicle Make", sort = alt.EncodingSortField(field = "count", order = "descending")),
    alt.Y("fraction_of_total", title = "Fraction of Total Tickets")
).properties(
    title = "Most Frequently Ticketed Vehicle Makes in Chicago, IL"
)
chart_2
```

I think that car models that are more likely to belong to higher income people will be paid more promptly than those with low-income. Thus, cars that are more likely to belong to low-income individuals are more likely to have an outstanding balance

3.3 Make a plot for the number of tickets issued over time by adapting the Filled Step Chart example online

```{python}
df_ps1["issue_date"] = pd.to_datetime(df_ps1["issue_date"])
```

```{python}
group_issue_date = df_ps1.groupby(df_ps1["issue_date"].astype(str).str[:10]).size().reset_index(name = "count")
```

```{python}
import altair as alt

alt.data_transformers.disable_max_rows()

chart_3 = alt.Chart(group_issue_date).mark_area(
    color="hotpink",
    interpolate='step-after',
    line=True
).encode(
   alt.X("issue_date:T", title = "Date"),
    alt.Y("count:Q", title = "Ticket Count")
).properties(
    title = "Most Frequently Ticketed Vehicle Makes in Chicago, IL"
)
chart_3
```

3.4 Make a plot for the number of tickets issued by month and day by adapting the Annual Weather Heatmap example online.

```{python}
df_ps1["month"] = df_ps1["issue_date"].dt.month
df_ps1["day"] = df_ps1["issue_date"].dt.day

group_month_and_day = df_ps1.groupby(["month","day"]).size().reset_index(name = "count")
```

```{python}
chart_4 = alt.Chart(group_month_and_day).mark_rect().encode(
    alt.X("day:O").title("Day"),
    alt.Y("month:O").title("Month"),
    alt.Color("count:Q",title = "Number of Tickets", scale = alt.Scale(scheme = "blues")),
    tooltip=[
        alt.Tooltip("month:O", title="Month"),
        alt.Tooltip("day:O", title = "Day"),
        alt.Tooltip("count:Q", title="Number of Tickets"),
    ]
).properties(
    title = "Daily Traffic Violation Tickets Issued in Chicago, IL"
=======
title: "ps1"
format:
  html:
    code-fold: true
jupyter: python3
---

## Question 1
### 1
```{python}
import time
import pandas as pd

# Start the timer
start_time = time.time()

# Read the CSV file
df = pd.read_csv('data/parking_tickets_one_percent.csv')

# Stop the timer and calculate duration
duration = time.time() - start_time
print(f"Time taken to read the file: {duration:.2f} seconds")

# Assert statement to verify the number of rows
assert len(df) == 287458, f"Expected 287458 rows, but got {len(df)}"
```

### 2
```{python}
import os
import math

# Get the size of the sample file in bytes
file_size_bytes = os.path.getsize('data/parking_tickets_one_percent.csv')

# Convert to megabytes
file_size_mb = file_size_bytes / (1024 * 1024)
print(f"File size of 1% sample: {file_size_mb:.2f} MB")

# Use math to calculate the full dataset size (since the sample is 1% of the total dataset)
predicted_full_size_mb = file_size_mb * 100
print(f"Predicted full dataset size: {predicted_full_size_mb} MB")
```


### 3
```{python}
ordered_col_name = "issue_date"
print(f"Upon inspection, the dataset is sorted by column {ordered_col_name}.")

# Convert this "issue_date" column to datetime object
df[ordered_col_name] = pd.to_datetime(df[ordered_col_name])

# Subsetting the first 500 rows
subset_df = df.head(500)

# Function to check if a column is ordered
def is_column_ordered(df, column):
  '''
  Check whether a column is ordered

  Args:
    1) df: dataframe
    2) column: column to check
  '''

  # return df[column].is_monotonic

# Test if "issue_date" is ordered
is_ordered = is_column_ordered(subset_df, ordered_col_name)
print(f"Is the column '{ordered_col_name}' ordered? {is_ordered}")
```

## Question 2
### 1
```{python}
n_tickets_sample = len(df[df['issue_date'].dt.year == 2017])
print(f"There are in total {n_tickets_sample} in the data in 2017")

n_tickets = n_tickets_sample * 100
print(f"This implies that there may be in total {n_tickets} in the full data in 2017.")

print("According to the ProPublica article, more than 3 million tickets were issueed each year for parking, vehicle compliance, and automated traffic camera violations")

print("Yes, there is a meaningful difference: The difference between your estimate (2.24 million) and the ProPublica figure (3 million) is around 760,000 tickets, which represents approximately a 25% difference. Given that the ProPublica data covers a broad range of years and mentions more than 3 million tickets per year, this discrepancy could be due to variations in ticket issuance rates year by year. However, a 25% difference is substantial enough to warrant attention and may suggest either: A lower ticket issuance rate specifically in 2017; 2) Incomplete or non-representative sampling in your dataset.")

```


### 2
```{python}
import matplotlib.pyplot as plt

# Plotting the bar graph
plt.figure(figsize=(10, 6))
df['violation_description'].value_counts().nlargest(20).plot(kind='bar')

# Formatting the graph
plt.title('Top 20 Most Frequent Violation Types', fontsize=16)
plt.xlabel('Violation Description', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)  # Adjust x-ticks for readability
plt.tight_layout()

# Show the plot
plt.show()
```


## Question 3
### 1

| **Variable Name**          | **Data Type(s)**       |
|----------------------------|------------------------|
| `ticket_number`             | Nominal (N)            |
| `issue_date`                | Temporal (T)           |
| `violation_location`        | Nominal (N)            |
| `license_plate_number`      | Nominal (N)            |
| `license_plate_state`       | Nominal (N)            |
| `license_plate_type`        | Nominal (N)            |
| `zipcode`                   | Nominal (N)            |
| `violation_code`            | Nominal (N) |
| `violation_description`     | Nominal (N)            |
| `unit`                      | Nominal (N)            |
| `unit_description`          | Nominal (N)            |
| `vehicle_make`              | Nominal (N)            |
| `fine_level1_amount`        | Quantitative (Q)       |
| `fine_level2_amount`        | Quantitative (Q)       |
| `current_amount_due`        | Quantitative (Q)       |
| `total_payments`            | Quantitative (Q)       |
| `ticket_queue`              | Nominal (N)            |
| `ticket_queue_date`         | Temporal (T)           |
| `notice_level`              | Ordinal (O)            |
| `hearing_disposition`       | Nominal (N)            |
| `notice_number`             | Nominal (N)            |
| `officer`                   | Nominal (N)            |
| `address`                   | Nominal (N)            |

### 2
```{python}
# Calculate fraction of tickets marked as paid for each vehicle make
paid_fraction = df[['vehicle_make', 'ticket_queue']].groupby('vehicle_make')['ticket_queue'].apply(lambda x: (x == 'Paid').mean()).sort_values(ascending=False)

# Plotting the results
plt.figure(figsize=(10, 6))
paid_fraction.plot(kind='bar')

# Formatting the graph
plt.title('Fraction of Tickets Marked as Paid for Each Vehicle Make', fontsize=16)
plt.xlabel('Vehicle Make', fontsize=12)
plt.ylabel('Fraction of Paid Tickets', fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)  # Adjust x-ticks for readability
plt.tight_layout()

# Show the plot
plt.show()
```
I think this could be related to **wealth correlation**. Specifically, owners of luxury or expensive vehicles might be more likely to pay their tickets promptly, as they may have more financial resources to do so.


### 3
```{python}
import altair as alt

# Group data by day or month to count the number of tickets issued over time
tickets_over_time = df.groupby(df['issue_date'].dt.date).size().reset_index(name='count')

# Convert 'issue_date' to string to avoid serialization issues
tickets_over_time['issue_date'] = tickets_over_time['issue_date'].astype(str)

# Create the filled step chart with Altair
chart = alt.Chart(tickets_over_time).mark_area(
    color="lightblue",
    interpolate='step-after',
    line=True
).encode(
    x='issue_date:T',  # Temporal encoding for the date
    y='count:Q'  # Quantitative encoding for the count of tickets
).properties(
    title='Number of Parking Tickets Issued Over Time'
)

chart.show()
```

### 4
```{python}
# Extract month and day for the heatmap
df['month'] = df['issue_date'].dt.month
df['day'] = df['issue_date'].dt.day

# Group data by month and day to get the number of tickets issued on each day
tickets_per_day = df.groupby(['month', 'day']).size().reset_index(name='count')

# Create the heatmap using Altair
heatmap = alt.Chart(tickets_per_day, title="Tickets Issued by Month and Day").mark_rect().encode(
    alt.X("day:O", title="Day"),
    alt.Y("month:O", title="Month"),
    alt.Color("count:Q", title="Number of Tickets"),
    tooltip=[
        alt.Tooltip("month:O", title="Month"),
        alt.Tooltip("day:O", title="Day"),
        alt.Tooltip("count:Q", title="Number of Tickets"),
    ]
).properties(
    width=600,  # Set chart width
    height=400  # Set chart height
>>>>>>> 4297bcb11075f0960844eca522264831d9cfe2a0
).configure_view(
    step=13,
    strokeWidth=0
).configure_axis(
<<<<<<< HEAD
    domain=False
)
chart_4
```

3.5 Subset to the five most common types of violations. Make a plot for the number of tickets issued over time by adapting the Lasagna Plot example online.

```{python}

violation_count_top_5 = df_ps1["violation_description"].value_counts().nlargest(5)

top_5_violation_converted = violation_count_top_5.index.tolist()

df_top_5_violations = df_ps1[df_ps1['violation_description'].isin(top_5_violation_converted)]

```

```{python}

df_top_5_violations.loc[:, "date_only_no_time"] = df_top_5_violations["issue_date"].astype(str).str[:10]

top_5_day_and_violation = df_top_5_violations.groupby(["date_only_no_time", "violation_description"]).size().reset_index(name = "count")

top_5_day_and_violation['date_only_no_time'] = top_5_day_and_violation['date_only_no_time'].astype(str)

```

```{python}
chart_5 = alt.Chart(top_5_day_and_violation, width = 500, height = 500).mark_rect().encode(
    x = alt.X("date_only_no_time:O", title = "Time",
        axis = alt.Axis(labelAngle = 0, labelOverlap = "greedy")),
    y = alt.Y("violation_description:N", title = "Violation Description",
       axis = alt.Axis( labelAngle = 0, labelOverlap = "greedy")),
    color = alt.Color("count:Q", title = "Ticket Count"),
    tooltip = [
        alt.Tooltip("date_only_no_time:O", title = "Time"),
        alt.Tooltip("violation_description:N", title = "Violation Description"),
        alt.Tooltip("count:Q", title = "Ticket Count")
    ]
).properties(
    title = "Top 5 Traffic Violations in Chicago,IL over time"
    )
chart_5
```

3.6



3.7
=======
    domain=False,
    grid=False
)

# Show the heatmap
heatmap.show()
```

### 5
```{python}
# Subset to the five most common violation types
top_violations = df['violation_description'].value_counts().nlargest(5).index
df_subset = df[df['violation_description'].isin(top_violations)]

# Group data by violation type and time (e.g., month/year) to get the number of tickets issued
df_subset['yearmonth'] = df_subset['issue_date'].dt.to_period('M')  # Year and month
tickets_by_time = df_subset.groupby(['violation_description', 'yearmonth']).size().reset_index(name='count')

# Convert 'yearmonth' back to datetime for Altair
tickets_by_time['yearmonth'] = tickets_by_time['yearmonth'].dt.to_timestamp()

# Create the Lasagna Plot using Altair
color_condition = alt.condition(
    "month(datum.yearmonth) == 1 && date(datum.yearmonth) == 1",
    alt.value("black"),
    alt.value(None),
)

lasagna_plot = alt.Chart(tickets_by_time).mark_rect().encode(
    alt.X("yearmonth:T")
        .title("Time")
        .axis(
            format="%Y-%m",  # Format as year and month
            labelAngle=0,
            labelOverlap=False,
            labelColor=color_condition,
            tickColor=color_condition,
        ),
    alt.Y("violation_description:N").title(None),
    alt.Color("count:Q").title("Number of Tickets")
).properties(
    width=600,  # Set chart width
    height=200  # Set chart height
)

# Show the plot
lasagna_plot.show()
```

### 6
1. Filled Step Chart
  - Pros:
    1. Good for time series analysis (easy to visualize changes)
    2. Appropriate for continuous data
    3. The step pattern makes it easy to understand when and how of pattern changes over time.
  - Cons:
    1. There is no breakdown by category (focusing only on the overall number).
    2. Not suitable for comparing days or months. 

2. Annual Weather Heatmap
  - Pros:
  	1. Great for spotting seasonality
    2. Detailed day-by-day view
    3. Good color intensity, which helps identify “hot spots” of activities
  - Cons:
    1. Difficult to compare categories
    2. Overwhelming for large datasets (especially when having too many data points)

3. Lasagna Plot
  - Pros:
    1. Good for comparing categories
    2. Good color intensity, which helps identify “hot spots” of activities
    3. Combines both time trends and category comparisons
  - Cons
    1. More complex to interpret
    2. Potential for oversaturation (with too many categories or too much data)

### 7
If the main goal is to show that the enforcement of violations is not evenly distributed over time, Lasagna Plot would be the best choice. 

First, it shows both time and categories. The lasagna plot doesn’t just show overall ticket issuance. It allows you to see how different violation types are enforced differently over time. This is key when the lesson is about uneven distribution.

Second, it shows category-level details. If some types of violations are enforced more rigorously at specific times, the lasagna plot will make this clear.

Finally, the use of color allows for quick identification of time periods with heightened enforcement activity for specific types of violations, making unevenness clear at both a temporal and categorical level.
>>>>>>> 4297bcb11075f0960844eca522264831d9cfe2a0


